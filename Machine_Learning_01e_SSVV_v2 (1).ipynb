{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Machine_Learning_01e_SSVV_v2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KG1mXcctJRN",
        "colab_type": "text"
      },
      "source": [
        "# <font color='blue'>Ensino Einstein</font>\n",
        "# <font color='blue'>Ciência de Dados na área da Saúde</font>\n",
        "## <font color='blue'>Machine Learning em Python</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RNjGLb2tJRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url = 'images/processo_DataScience.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkGsuMlPtJRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krKRZr12tJRY",
        "colab_type": "text"
      },
      "source": [
        "### Definição do Problema de Negócio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxmDqkEjtJRZ",
        "colab_type": "text"
      },
      "source": [
        "Criação de um modelo preditivo que seja capaz de prever a chance de uma pessoa ser internada durante o processo de Triagem.\n",
        "\n",
        "Primeiramente apenas os Sinais Vitais, posteriormente os Sinais Vitais em conjunto com as demais variáveis já pré selecionadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqrot5vVtJRZ",
        "colab_type": "text"
      },
      "source": [
        "Dataset: HIAE_SSVV_v01.csv, HIAE_SSVV_v02.csv & HIAE_Triagem_v03.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZwOgS3ZtJRa",
        "colab_type": "text"
      },
      "source": [
        "Este dataset descreve os registros médicos entre pacientes do HIAE após a passagem pela Triagem, onde agora cada registro já possui a informação se o paciente internou ou não, de acordo com pré-processamento já realizado anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBp1MhCOtJRb",
        "colab_type": "text"
      },
      "source": [
        "### Informações sobre os atributos:\n",
        "\n",
        "1. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8dRQf59tJRc",
        "colab_type": "text"
      },
      "source": [
        "### Extraindo e Carregando os Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvYCasSWtJRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carregando arquivo .csv\n",
        "import pandas as pd\n",
        "file = 'data/HIAE_SSVV_v02.csv'\n",
        "df_raw = pd.read_csv(file, sep = \";\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fox3kQ5utJRh",
        "colab_type": "text"
      },
      "source": [
        "### Análise Exploratória de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5y9KdH-tJRh",
        "colab_type": "text"
      },
      "source": [
        "#### Estatística Descritiva"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvFievhUtJRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizando as primeiras 10 linhas\n",
        "df_raw.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQIMG3ZstJRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizando as dimensões\n",
        "df_raw.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXviv-5ctJRq",
        "colab_type": "text"
      },
      "source": [
        "Preparando o dataset mantendo apenas as variáveis numéricas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ZIbE3WtJRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lista as colunas para referência\n",
        "df_raw.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gkoh9wltJRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colunas para deletar\n",
        "col2del = ['T', 'PAd', 'PAs', 'FR', 'P', 'SpO2', 'T_str', 'PA_str', 'FR_str', 'P_str', 'SpO2_str']\n",
        "\n",
        "# Deleta as colunas\n",
        "df_raw.drop(col2del, axis = 1, inplace = True) # 'axis' = 0 (row) | axis = 1 (column)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er5SYKGCtJRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tipo de dados de cada atributo\n",
        "df_raw.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50xDf9B4tJR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sumário estatístico\n",
        "df_raw.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVuDoXv1tJR4",
        "colab_type": "text"
      },
      "source": [
        "Em problemas de classificação pode ser necessário balancear as classes. Aqui existe uma clara desproporção entre as classes 0 (paciente não interna) e 1 (paciente interna)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "311hB5yktJR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribuição das classes\n",
        "df_raw.groupby('Interna_num').size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbwZakPqtJR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Balanceamento (Down-sample Majority Class)\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = df_raw[df_raw.Interna_num == 0]\n",
        "df_minority = df_raw[df_raw.Interna_num == 1]\n",
        " \n",
        "# Upsample minority class\n",
        "df_majority_downsampled = resample(df_majority, \n",
        "                                   replace = False,    # sample without replacement\n",
        "                                   n_samples = 1955,   # to match minority class\n",
        "                                   random_state = 123) # reproducible results\n",
        " \n",
        "# Combine minority class with downsampled majority class\n",
        "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
        " \n",
        "# Display new class counts\n",
        "df_downsampled.Interna_num.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_EsNme8tJSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_downsampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_yvp5uRtJSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dados = df_downsampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y9Pk0QItJSG",
        "colab_type": "text"
      },
      "source": [
        "A correlação é o relacionamento entre 2 variáveis. O método mais comum para calcular correlação é o método de Pearson, que assume uma distribuição normal dos dados. Correlação de -1 mostra uma correlação negativa, enquanto uma correlação de +1 mostra uma correlação positiva. Uma correlação igual a 0 mostra que não há relacionamento entre as variáveis.\n",
        "\n",
        "(!!!) Alguns algoritmos como Regressão Linear e Regressão Logística podem apresentar problemas de performance se houver atributos altamente correlacionados (colineares)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK86lE_3tJSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Correlação de Pearson\n",
        "dados.corr(method = 'pearson')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDph0f3FtJSK",
        "colab_type": "text"
      },
      "source": [
        "#### Visualização com Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XCJNOx4tJSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Gráficos em janela separada\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZP8NigOtJSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Matriz de Correlação (- nome das variáveis)\n",
        "correlations = dados.corr()\n",
        "\n",
        "# Plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(correlations, vmin = -1, vmax = 1)\n",
        "fig.colorbar(cax)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_HHlKKntJSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dados.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjLBuo17tJSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colunas = ['T_num', 'PA_num', 'FR_num', 'P_num', 'SpO2_num', 'Interna_num']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inqHJDTItJSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Matriz de Correlação (+ nome das variáveis)\n",
        "correlations = dados.corr()\n",
        "\n",
        "# Plot\n",
        "import numpy as np\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(correlations, vmin = -1, vmax = 1)\n",
        "fig.colorbar(cax)\n",
        "ticks = np.arange(0, 6, 1)\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_yticks(ticks)\n",
        "ax.set_xticklabels(colunas)\n",
        "ax.set_yticklabels(colunas)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeQn0M63tJSa",
        "colab_type": "text"
      },
      "source": [
        "Skew (ou simetria) se refere a distribuição dos dados que é assumida ser normal ou Gaussiana (bell curve). Muitos algoritmos de Machine Learning consideram que os dados possuem uma distribuição normal. O conhecendo sobre a simetria dos dados permite que você faça uma preparação adequada e entregue o que o algoritmo espera receber."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtel6OFDtJSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verificando o Skew de cada atributo\n",
        "dados.skew()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww3uh3WNtJSe",
        "colab_type": "text"
      },
      "source": [
        "Density Plots são outra forma de visualizar a distribuição dos dados para cada atributo, apresentando curvas através do topo dos bins de um histograma. Assim, a distribuição dos dados usando um Density Plot pode se tornar um pouco mais identificável."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwFUGoY2tJSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Density Plot\n",
        "dados.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL3_LY0ptJSh",
        "colab_type": "text"
      },
      "source": [
        "### Preparando os Dados para Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpnnSRSMtJSi",
        "colab_type": "text"
      },
      "source": [
        "Muitos algoritmos esperam receber os dados em um formato específico. Você deve preparar os dados de acordo com uma estrutura que seja adequada ao algoritmo que você irá utilizar, o que pode requerer transformações diferentes nos dados. Ainda, é possível que em alguns casos, bons resultados sejam obtidos sem um trabalho de pré-processamento, mas é uma boa prática criar diferentes transformações dos dados para realizar testes em diferentes algoritmos de Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNYZPU9ftJSi",
        "colab_type": "text"
      },
      "source": [
        "#### Normalização - Método 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzgS7x6ktJSk",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLbwf9gztJSl",
        "colab_type": "text"
      },
      "source": [
        "Uma das primeiras tarefas dentro do pré-processamento é colocar os dados em uma mesma escala. Muitos algoritmos de Machine Learning vão se beneficiar desse processamento e produzir resultados melhores. Esta etapa também é chamada de Normalização e significa colocar os dados em uma escala entre 0 e 1.\n",
        "\n",
        "(!!!) Importantíssimo para o processo de otimização, assim como para algoritmos como Regressão e Redes Neurais e algoritmos que usam medidas de distância, como KNN.\n",
        "\n",
        "O scikit-learn possui uma função para esse processamento, chamada MinMaxScaler()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gGuuIbuYtJSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformando os dados para a mesma escala (entre 0 e 1)\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input (X) e output (Y)\n",
        "X = array[:, 0:5]\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Gerando a nova escala (normalizando os dados)\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "rescaledX = scaler.fit_transform(X)\n",
        "\n",
        "# Sumarizando os dados transformados\n",
        "print(\"Dados Originais: \\n\\n\", dados.values)\n",
        "print(\"\\nDados Normalizados: \\n\\n\", rescaledX[0:5, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-kd3yeqtJSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualização com Seaborn\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "sns.distplot(rescaledX[:, 1], fit = stats.laplace, kde = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLQp4mTjtJSr",
        "colab_type": "text"
      },
      "source": [
        "#### Normalização - Método 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKfRJ4ihtJSr",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSHOPsEPtJSs",
        "colab_type": "text"
      },
      "source": [
        "No scikit-learn, Normalização se refere a ajustar a escala de cada observação (linha) de modo que ela tenha comprimento igual a 1 (chamado vetor de comprimento 1 em álgebra linear).\n",
        "\n",
        "(!!!) Importantíssimo quando temos Datasets esparsos (com muitos zeros) e atributos com escala muito variada, assim como para algoritmos de Redes Neurais ou que usam medida de distância, como KNN.\n",
        "\n",
        "O scikit-learn possui uma função para esta etapa, chamada Normalizer()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qe_VcfetJSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizando os dados (comprimento igual a 1)\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:3]\n",
        "Y = array[:, 3]\n",
        "\n",
        "# Gerando os dados normalizados\n",
        "scaler = Normalizer().fit(X)\n",
        "normalizedX = scaler.transform(X)\n",
        "\n",
        "# Sumarizando os dados transformados\n",
        "print(\"Dados Originais: \\n\\n\", dados.values)\n",
        "print(\"\\nDados Normalizados: \\n\\n\", normalizedX[0:5, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DGHZer1tJSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(normalizedX[:, 1], fit = stats.laplace, kde = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H10KRJR_tJSx",
        "colab_type": "text"
      },
      "source": [
        "#### Padronização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkyashGAtJSx",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4DDQnX3tJSy",
        "colab_type": "text"
      },
      "source": [
        "Padronização é a técnica para transformar os atributos com distribuição Gaussiana (normal) e diferentes médias e desvios padrão em uma distribuição Gaussiana com a média igual a 0 e desvio padrão igual a 1.\n",
        "\n",
        "(!!!) Importantíssimo para algoritmos que esperam que os dados estejam com uma distribuição Gaussiana, como Regressão Linear, Regressão Logística e Linear Discriminant Analysis (LDA). Funciona bem quando os dados já estão na mesma escala.\n",
        "\n",
        "O scikit-learn possui uma função para esta etapa, chamada StandardScaler()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6tzAfXitJSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:5]\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Gerando o novo padrão\n",
        "scaler = StandardScaler().fit(X)\n",
        "standardX = scaler.transform(X)\n",
        "\n",
        "# Sumarizando os dados transformados\n",
        "print(\"Dados Originais: \\n\\n\", dados.values)\n",
        "print(\"\\nDados Padronizados: \\n\\n\", standardX[0:5,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0BpJcC6tJS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(standardX[:, 1], fit = stats.laplace, kde = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI3HxrNmtJS5",
        "colab_type": "text"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ8leQMutJS6",
        "colab_type": "text"
      },
      "source": [
        "Os atributos presentes no seu Dataset e que você utiliza nos dados de treino terão grande influência na precisão e resultado do seu modelo preditivo.\n",
        "\n",
        "(!!!) Importantíssimo notar que atributos irrelevantes geram um impacto negativo na performance, enquanto que atributos colineares podem afetar o grau de acurácia do modelo.\n",
        "\n",
        "O scikit-learn possui funções que automatizam o trabalho de extração e seleção de variáveis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRcOTU3htJS6",
        "colab_type": "text"
      },
      "source": [
        "A etapa de Feature Selection é onde selecionamos os atributos (variáveis) que serão melhores candidatas a variáveis preditoras. O Feature Selection nos ajuda a reduzir o overfitting (quando o algoritmo aprende demais), aumenta a acurácia do modelo e reduz o tempo de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdoWEeTotJS7",
        "colab_type": "text"
      },
      "source": [
        "#### Eliminação Recursiva de Atributos (Recursive Feature Elimination - RFE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfwnjYvMtJS7",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkKkeCattJS8",
        "colab_type": "text"
      },
      "source": [
        "Técnica para seleção de atributos que recursivamente remove os atributos e constrói o modelo com os atributos remanescentes. Utiliza a acurácia do modelo para identificar os atributos que mais contribuem para prever a variável alvo. Em inglês esta técnia é chamada Recursive Feature Elimination (RFE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1QUjAVmtJS8",
        "colab_type": "text"
      },
      "source": [
        "O exemplo abaixo utiliza a técnica de eliminação recursiva de atributos com um algoritmo de Regressão Logística para selecionar as 3 melhores variáveis preditoras. O RFE selecionou as variáveis PA_num, FR_num e SpO2_num, as quais marcadas como True em \"Atributos Selecionados\" e com valor 1 em \"Ranking dos Atributos\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkYl3Ep0tJS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Eliminação Recursiva de Variáveis\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:5]\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Criação do modelo\n",
        "modelo = LogisticRegression()\n",
        "\n",
        "# RFE\n",
        "rfe = RFE(modelo, 3)\n",
        "fit = rfe.fit(X, Y)\n",
        "\n",
        "# Print dos resultados\n",
        "print(\"Variáveis Preditoras:\", dados.columns[0:8])\n",
        "print(\"Variáveis Selecionadas: %s\" % fit.support_)\n",
        "print(\"Ranking dos Atributos: %s\" % fit.ranking_)\n",
        "print(\"Número de Melhores Atributos: %d\" % fit.n_features_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F6wu4pRtJS_",
        "colab_type": "text"
      },
      "source": [
        "#### Método Ensemble para Seleção de Variáveis (ExtraTreesClassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdC_C9uOtJTA",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGR_DTUCtJTA",
        "colab_type": "text"
      },
      "source": [
        "Bagged Decision Trees, como o algoritmo RandomForest (esses são chamados de Métodos Ensemble), podem ser usados para estimar a importância de cada atributo, retornando um score para cada um dos mesmos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d0FNVxWtJTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importância do Atributo com o Extra Trees Classifier\n",
        "\n",
        "# Import dos Módulos\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:5]\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Criação do Modelo - Feature Selection\n",
        "modelo = ExtraTreesClassifier()\n",
        "modelo.fit(X, Y)\n",
        "\n",
        "# Print dos Resultados\n",
        "print(dados.columns[0:5])\n",
        "print(modelo.feature_importances_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtefDflvtJTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colunas para deletar\n",
        "col2del = ['T_num', 'P_num']\n",
        "\n",
        "# Deleta as colunas\n",
        "df_raw.drop(col2del, axis = 1, inplace = True) # 'axis' = 0 (row) | axis = 1 (column)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0SUdsuttJTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dados = df_raw\n",
        "dados"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JzjCozVtJTI",
        "colab_type": "text"
      },
      "source": [
        "### Amostragem - Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w66FG-LStJTJ",
        "colab_type": "text"
      },
      "source": [
        "A avaliação do modelo é uma estimativa de quão bem o algoritmo será capaz de prever em novos dados. Para tanto, devemos fazer previsões em dados que você já conhece o resultado, mas que o modelo não. Existem diversas técnicas para isso e estudaremos duas aqui: Conjunto de dados de treino e de teste e Cross Validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX3HOHL1tJTK",
        "colab_type": "text"
      },
      "source": [
        "#### Dados de Treino e de Teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7ZX7260tJTL",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnUb_LNItJTL",
        "colab_type": "text"
      },
      "source": [
        "Método mais utilizado para avaliar a performance de um algoritmo de Machine Learning. Divide-se os dados originais em dados de treino e de teste. Treina-se o algoritmo nos dados de treino e então se faze as previsões nos dados de teste e avalia-se o resultado. A divisão dos dados depende do seu Dataset, mas proporções entre 70/30 (treino/teste) e 65/35 (treino/teste) são muito frequentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMxlDlAAtJTM",
        "colab_type": "text"
      },
      "source": [
        "Este método é bem veloz e ideal para conjuntos de dados muito grandes. O ponto negativo é a possibilidade de alta variância."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyeT01iTtJTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Avaliação usando dados de treino e de teste\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # **\n",
        "X = rescaledX     # ***\n",
        "# X = normalizedX   # *\n",
        "# X = standardX     # **\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo o tamanho das amostras\n",
        "teste_size = 0.33\n",
        "\n",
        "# Garante que os resultados podem ser reproduzidos\n",
        "# Isso é importante para comparar a acurácia com outros algoritmos de Machine Learning.\n",
        "seed = 7\n",
        "\n",
        "# Criando os conjuntos de dados de treino e de teste\n",
        "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Criação do modelo\n",
        "modelo = LogisticRegression()\n",
        "\n",
        "# Treinamento do modelo\n",
        "modelo.fit(X_treino, Y_treino)\n",
        "\n",
        "# Score do modelo nos dados de teste\n",
        "result = modelo.score(X_teste, Y_teste)\n",
        "print(\"Acurácia nos Dados de Teste: %.3f%%\" % (result * 100.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPBIVQCetJTQ",
        "colab_type": "text"
      },
      "source": [
        "#### Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_8MzsADtJTQ",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFqG59kdFKYb",
        "colab_type": "code",
        "outputId": "88665e4d-2b74-430d-a013-6f2ec39a34c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk5BRz_GtJTR",
        "colab_type": "text"
      },
      "source": [
        "Cross Validation é uma técnica que pode ser utilizada para avaliar a performance de um modelo com menos variância que a técnica de dividir os dados em treino/teste. Com esta técnica dividimos os dados em partes chamadas k-folds. Cada parte é chamada fold e o algoritmo é treinado em k-1 folds, isso é, cada fold é usado no treinamento de forma repetida, um fold por vez. Após executar o processo em k-1 folds, sumariza-se a performance em cada fold usando a média e o desvio padrão. O resultado é normalmente mais confiável e oferece maior acurácia ao modelo. A chave deste processo está em definir o correto valor de k, de modo que o número de folds represente adequadamente o número de repetições necessárias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq7VxidZtJTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url = 'images/cross-validation.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN_yTBBatJTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Avaliação usando Cross Validation\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "X = normalizedX   # **\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para os folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = LogisticRegression()\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Usamos a média e o desvio padrão\n",
        "print(\"Acurácia Final: %.3f%%\" % (resultado.mean() * 100.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGs_ExONtJTY",
        "colab_type": "text"
      },
      "source": [
        "### Avaliando a Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5bYbKoDtJTZ",
        "colab_type": "text"
      },
      "source": [
        "As métricas que você escolhe para avaliar a performance do modelo vão influenciar a forma como a performance é medida e comparada com modelos criados com outros algoritmos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6763vHRtJTZ",
        "colab_type": "text"
      },
      "source": [
        "Vamos utilizar o mesmo algoritmo, mas com métricas diferentes e assim comparar os resultados. A função cross_validation.cross_val_score() será usada para avaliar a performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWNdtEgWtJTa",
        "colab_type": "text"
      },
      "source": [
        "#### Métricas para Algoritmos de Classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8lzAt6RtJTa",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_vYncgwtJTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Acurácia\n",
        "# Número de previsões corretas. É útil apenas quando existe um balanceamento entre as classes.\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "X = normalizedX   # **\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = LogisticRegression()\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold, scoring = 'accuracy')\n",
        "\n",
        "# Print dos resultados\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P13t5XaQtJTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Curva ROC \n",
        "# A Curva ROC permite analisar a métrica AUC (Area Under the Curve).\n",
        "# Essa é uma métrica de performance para classificação binária, em que podemos definir as classes em positiavs e negativas.\n",
        "# Problemas de classificação binária são um trade-off sentre Sensitivity e Specifity.\n",
        "# Sensitivity é a taxa de verdadeiros positivos (TP). Ese é o número de instâncias positivas da primeira classe que foram previstas corretamente.\n",
        "# Specifity é a taxa de verdadeiros negativos (TN). Esse é o número de instâncias da segunda classe que foram previstas corretamente.\n",
        "# Valores acima de 0.5 indicam uma boa taxa de previsão.\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # ***\n",
        "# X = rescaledX     # **\n",
        "X = normalizedX   # ****\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Criando o modelo\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(model, X, Y, cv = kfold, scoring = 'roc_auc')\n",
        "\n",
        "# Print do resultado\n",
        "print(\"AUC: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvJY7pCAtJTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.patches as patches\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from scipy import interp\n",
        "\n",
        "# plot arrows\n",
        "fig1 = plt.figure(figsize=[12,12])\n",
        "ax1 = fig1.add_subplot(111,aspect = 'equal')\n",
        "ax1.add_patch(\n",
        "    patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
        "    )\n",
        "ax1.add_patch(\n",
        "    patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
        "    )\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0,1,100)\n",
        "i = 1\n",
        "for train,test in kfold.split(X,Y):\n",
        "    prediction = model.fit(pd.DataFrame(X).iloc[train],pd.DataFrame(Y).iloc[train]).predict_proba(pd.DataFrame(X).iloc[test])\n",
        "    fpr, tpr, t = roc_curve(Y[test], prediction[:, 1])\n",
        "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "    i= i+1\n",
        "\n",
        "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
        "         label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
        "plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZIJKS8ctJTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion Matrix\n",
        "# Permite verificar a acurácia em um formato de tabela\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5]\n",
        "# X = rescaledX\n",
        "X = normalizedX\n",
        "# X = standardX\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo o tamanho do conjunto de dados\n",
        "teste_size = 0.33\n",
        "seed = 7\n",
        "\n",
        "# Dividindo os dados em treino e teste\n",
        "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Criando o modelo\n",
        "model = LogisticRegression()\n",
        "model.fit(X_treino, Y_treino)\n",
        "\n",
        "# Fazendo as previsões e construindo a Confusion Matrix\n",
        "previsoes = model.predict(X_teste)\n",
        "matrix = confusion_matrix(Y_teste, previsoes)\n",
        "\n",
        "# Imprimindo a Confusion Matrix\n",
        "print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncW29t7RtJTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Relatório de Classificação\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5]\n",
        "# X = rescaledX\n",
        "X = normalizedX\n",
        "# X = standardX\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo o tamanho do conjunto de dados\n",
        "teste_size = 0.33\n",
        "seed = 7\n",
        "\n",
        "# Dividindo os dados em treino e teste\n",
        "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
        "\n",
        "# Import dos módulos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = LogisticRegression()\n",
        "modelo.fit(X_treino, Y_treino)\n",
        "\n",
        "# Fazendo as previsões e construindo o relatório\n",
        "previsoes = model.predict(X_teste)\n",
        "report = classification_report(Y_teste, previsoes)\n",
        "\n",
        "# Imprimindo o relatório\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw6MY0UetJTp",
        "colab_type": "text"
      },
      "source": [
        "## Algoritmos de Classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS1HhxL1tJTp",
        "colab_type": "text"
      },
      "source": [
        "Não há como como saber qual algoritmo vai funcionar melhor na construção de um modelo antes de o testarmos com um Dataset. O ideal é testar alguns algoritmos e então escolher o que fornece melhor nível de precisão (nesse caso, já que estamos com nossas classes balanceadas e estamos definindo essa estratégia). Vamos testar um conjunto de algoritmos de classificação nas mesmas condições e validarmos nossas conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3FoQ2r-tJTq",
        "colab_type": "text"
      },
      "source": [
        "### Regressão Logística"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF3zbH3CtJTq",
        "colab_type": "text"
      },
      "source": [
        "Algoritmo Linear. O algoritmo de Regressão Logística assume que seus dados estão em uma Distribuição Normal para valores numéricos que podem ser modelados com classificação binária."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkkFxRAFtJTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "X = normalizedX   # **\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = LogisticRegression()\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xni9AXUvtJTu",
        "colab_type": "text"
      },
      "source": [
        "### Linear Discriminant Analysis (LDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owmjj2DQtJTu",
        "colab_type": "text"
      },
      "source": [
        "Algoritmo Linear. Técnica estatística para classificação binária. Também assume que os dados estão em Distribuição Normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWukvaTGtJTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:5] # **\n",
        "# X = rescaledX     # **\n",
        "# X = normalizedX   # *\n",
        "# X = standardX     # **\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7bh9cbtJTw",
        "colab_type": "text"
      },
      "source": [
        "### KNN (K-Nearest Neighbors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjPewlibtJTz",
        "colab_type": "text"
      },
      "source": [
        "Algoritmo Não-Linear que utiliza uma métrica de distância para encontrar o valor de K mais adequado as instâncias do Dataset de treino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GmNn52jtJT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "# X = normalizedX   # *\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "random_state = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = random_state)\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = KNeighborsClassifier()\n",
        "\n",
        "# Cross Validation\n",
        "results = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBCtM2T2tJT2",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JSEMHFetJT2",
        "colab_type": "text"
      },
      "source": [
        "Algoritmo Não-Linear. Calcula a probabilidade de cada classe e a probabilidade condicional de cada classe dado uma variável de entrada. As probabilidades são então estimadas para os novos dados e multiplicadas, assumindo que são independentes (suposição simples ou Naive). Assume dados em distirbuição Gaussiana (Normal)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om01SuHftJT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "X = normalizedX   # **\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = GaussianNB()\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSo7lKPStJT7",
        "colab_type": "text"
      },
      "source": [
        "### CART (Classification and Regression Trees)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ1X7SQjtJT7",
        "colab_type": "text"
      },
      "source": [
        "Algoritmo Não-Linear. O algoritmo CART constrói uma árvore binária a partir do Dataset de treino. Cada atributo e cada valor de cada atributo são avaliados com o objetivo de reduzir a função de custo (Cost Function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbt6FQ_1tJT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "X = normalizedX   # **\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = DecisionTreeClassifier()\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ9DtMTntJUA",
        "colab_type": "text"
      },
      "source": [
        "## Seleção do Modelo Preditivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzciCOhetJUA",
        "colab_type": "text"
      },
      "source": [
        "Veremos que os algoritmos Naive Bayes e CART apresentaram o melhor nível de precisão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pezh-JA2tJUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5]\n",
        "# X = rescaledX\n",
        "X = normalizedX\n",
        "# X = standardX\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Preparando a lista de modelos\n",
        "modelos = []\n",
        "modelos.append(('LR', LogisticRegression()))\n",
        "modelos.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "modelos.append(('NB', GaussianNB()))\n",
        "modelos.append(('KNN', KNeighborsClassifier()))\n",
        "modelos.append(('CART', DecisionTreeClassifier()))\n",
        "\n",
        "# Avaliando cada modelo em um loop\n",
        "resultados = []\n",
        "nomes = []\n",
        "\n",
        "for nome, modelo in modelos:\n",
        "    kfold = KFold(n_splits = num_folds, random_state = seed)\n",
        "    cv_results = cross_val_score(modelo, X, Y, cv = kfold, scoring = 'accuracy')\n",
        "    resultados.append(cv_results)\n",
        "    nomes.append(nome)\n",
        "    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "\n",
        "# Boxplot para comparar os algoritmos\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparação de Algoritmos de Classificação')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(resultados)\n",
        "ax.set_xticklabels(nomes)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JOz9aJztJUD",
        "colab_type": "text"
      },
      "source": [
        "### Otimização do Modelo - Ajuste de Hyperparâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kf1dvmWtJUD",
        "colab_type": "text"
      },
      "source": [
        "Todos os algoritmos de Machine Learning são parametrizados, o que significa que você pode ajustar a performance do seu modelo preditivo através do \"tuning\" (ajuste fino) dos parâmetros. \n",
        "\n",
        "(!!!) É importantíssimo encontrar a melhor combinação entre os parâmetros em cada algoritmo de Machine Learning, e esse processo também é chamado de Otimização de Hyperparâmetros.\n",
        "\n",
        "O scikit-learn oferece dois métodos para otimização automática dos parâmetros: Grid Search Parameter Tuning e Random Search Parameter Tuning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx1-u4GGtJUD",
        "colab_type": "text"
      },
      "source": [
        "#### Grid Search Parameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvBVzoihtJUE",
        "colab_type": "text"
      },
      "source": [
        "Este método realiza combinações entre todos os parâmetros do algoritmo, criando um grid. Vamos experimentar este método utilizando o algoritmo de Regressão Logística. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHRTXWrOtJUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "X = normalizedX   # **\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores que serão testados\n",
        "valores_grid = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
        "\n",
        "# Criando o modelo\n",
        "# modelo = LogisticRegression()\n",
        "modelo = LogisticRegression()\n",
        "\n",
        "# Criando o grid\n",
        "grid = GridSearchCV(estimator = modelo, param_grid = valores_grid)\n",
        "grid.fit(X, Y)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (grid.best_score_ * 100))\n",
        "print(\"Melhores Parâmetros do Modelo:\\n\", grid.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th6Z6KMqtJUH",
        "colab_type": "text"
      },
      "source": [
        "#### Random Search Parameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlXQvYVotJUI",
        "colab_type": "text"
      },
      "source": [
        "Este método gera amostras dos parâmetros dos algoritmos a partir de uma distribuição randômica uniforme para um número fixo de iterações. Um modelo é construído e testado para cada combinação de parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWsrGCWQtJUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "X = normalizedX   # **\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores que serão testados\n",
        "seed = 7\n",
        "iterations = 14\n",
        "\n",
        "# Definindo os valores que serão testados\n",
        "valores_grid = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = LogisticRegression()\n",
        "\n",
        "# Criando o grid\n",
        "rsearch = RandomizedSearchCV(estimator = modelo, \n",
        "                             param_distributions = valores_grid, \n",
        "                             n_iter = iterations, \n",
        "                             random_state = seed)\n",
        "rsearch.fit(X, Y)\n",
        "\n",
        "# Print dos resultados\n",
        "print(\"Acurácia: %.3f\" % (rsearch.best_score_ * 100))\n",
        "print(\"Melhores Parâmetros do Modelo:\\n\", rsearch.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uwpubZltJUL",
        "colab_type": "text"
      },
      "source": [
        "## Otimizando Performance com Métodos Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jAU7IOTtJUL",
        "colab_type": "text"
      },
      "source": [
        "Métodos Ensemble permitem aumentar consideravelmente o nível de precisão nas suas previsões. Vamos criar alguns dos Métodos Ensemble mais poderosos em Python, dos quais 3 principais para combinar previsões a partir de diferentes modelos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h923i31tJUM",
        "colab_type": "text"
      },
      "source": [
        "Bagging - Para construção de múltiplos modelos (normalmente do mesmo tipo) a partir de diferentes subsets no dataset de treino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Z7m8kxtJUN",
        "colab_type": "text"
      },
      "source": [
        "Boosting - Para construção de múltiplos modelos (normalmente do mesmo tipo), onde cada modelo aprende a corrigir os erros gerados pelo modelo anterior, dentro da sequência de modelos criados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO_uHVY3tJUN",
        "colab_type": "text"
      },
      "source": [
        "Voting - Para construção de múltiplos modelos (normalmente de tipos diferentes) e estatísticas simples (como a média) são usadas para combinar as previsões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUwihAgUtJUN",
        "colab_type": "text"
      },
      "source": [
        "### Bagged Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gcWbnN2tJUO",
        "colab_type": "text"
      },
      "source": [
        "Funciona bem quando existe alta variância nos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk1F4VSYtJUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:5] # **\n",
        "# X = rescaledX     # **\n",
        "# X = normalizedX   # *\n",
        "# X = standardX     # **\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Cria o modelo unitário (classificador fraco)\n",
        "cart = DecisionTreeClassifier()\n",
        "\n",
        "# Definindo o número de trees\n",
        "num_trees = 100\n",
        "\n",
        "# Criando o modelo bagging\n",
        "modelo = BaggingClassifier(base_estimator = cart, n_estimators = num_trees, random_state = seed)\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtVfM-DQtJUR",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-kwgP9vtJUR",
        "colab_type": "text"
      },
      "source": [
        "Random Forest é uma extensão do Baggig Decision Tree. Amostras do Dataset de treino são usadas com reposição, mas as árvores são criadas de uma forma que reduz a correlação entre classificadores individuais (Random Forest é um conjunto de árvores de decisão)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MncZMJletJUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:5] # **\n",
        "# X = rescaledX     # **\n",
        "# X = normalizedX   # *\n",
        "# X = standardX     # **\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Definindo o número de trees\n",
        "num_trees = 100\n",
        "max_features = 5\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = RandomForestClassifier(n_estimators = num_trees, max_features = max_features, random_state = seed)\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0kZNhTJtJUT",
        "colab_type": "text"
      },
      "source": [
        "### AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN_jdlgHtJUT",
        "colab_type": "text"
      },
      "source": [
        "Algoritmos baseados em Boosting Ensemble criam uma sequência de modelos que tentam corrigir os erros dos modelos anteriores dentro da sequência. Uma vez criados, os modelos fazem previsões que podem receber um peso de acordo com sua acurácia e os resultados são combinados para criar uma previsão única final. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFf8ykpatJUU",
        "colab_type": "text"
      },
      "source": [
        "O AdaBoost atribui pesos às instâncias no Dataset, definindo quão fácil ou difícil elas são para o processo de classificação, permitindo que o algoritmo tenha mais ou menos atenção às instâncias durante o processo de construção dos modelos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CENO03H6tJUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # **\n",
        "# X = rescaledX     # **\n",
        "# X = normalizedX   # *\n",
        "X = standardX     # **\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Definindo o número de trees\n",
        "num_trees = 30\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = AdaBoostClassifier(n_estimators = num_trees, random_state = seed)\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptO8SnDftJUW",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlCV0HMUtJUW",
        "colab_type": "text"
      },
      "source": [
        "Também chamado Stochastic Gradient Boosting, é um dos métodos Ensemble mais sofisticados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NdYTPANtJUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "# X = array[:, 0:5] # *\n",
        "# X = rescaledX     # *\n",
        "X = normalizedX   # **\n",
        "# X = standardX     # *\n",
        "Y = array[:, 5]\n",
        "\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Definindo o número de trees\n",
        "num_trees = 100\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Criando o modelo\n",
        "modelo = GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
        "\n",
        "# Print do resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2s1R4TTtJUZ",
        "colab_type": "text"
      },
      "source": [
        "### Voting Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcspC-YQtJUZ",
        "colab_type": "text"
      },
      "source": [
        "Este é um dos métodos Ensemble mais simples. Este método cria dois ou mais modelos separados a partir do Dataset de treino. O Classificador Voting então utiliza a média das previsões de cada sub-modelo para fazer as previsões em novos conjuntos de dados. As previsões de cada sub-modelo podem receber pesos, através de parâmetros definidos manualmente ou através de heurística."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXSeMdG3tJUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dos módulos\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "array = dados.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:, 0:5] # ***\n",
        "# X = rescaledX   # *\n",
        "# X = normalizedX # ***\n",
        "# X = standardX   # **\n",
        "Y = array[:, 5]\n",
        "\n",
        "# Definindo os valores para o número de folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Separando os dados em folds\n",
        "kfold = KFold(num_folds, True, random_state = seed)\n",
        "\n",
        "# Criando os modelos\n",
        "estimators = []\n",
        "\n",
        "modelo1 = LogisticRegression()\n",
        "estimators.append(('logistic', modelo1))\n",
        "\n",
        "modelo2 = DecisionTreeClassifier()\n",
        "estimators.append(('cart', modelo2))\n",
        "\n",
        "modelo3 = SVC()\n",
        "estimators.append(('svm', modelo3))\n",
        "\n",
        "# Criando o modelo ensemble\n",
        "ensemble = VotingClassifier(estimators)\n",
        "\n",
        "# Cross Validation\n",
        "resultado = cross_val_score(ensemble, X, Y, cv = kfold)\n",
        "\n",
        "# Resultado\n",
        "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r2wPny3tJUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}